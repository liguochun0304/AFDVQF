# -*- coding: utf-8 -*-
# @Time    : 2025/7/22 ä¸‹åˆ1:13
# @Author  : liguochun
# @FileName: train.py
# @Email   ï¼šliguochun0304@163.com
import json
import os
import random
from datetime import datetime

import numpy as np
import torch

from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup

from dataloader import MMPNERDataset, MMPNERProcessor, collate_fn
from model import MQSPNSetNER
from test import evaluate_model

script_dir = os.path.dirname(os.path.abspath(__file__))
STORAGE_ROOT = "/root/autodl-fs"
DATA_ROOT = os.path.join(STORAGE_ROOT, "data")


def set_seed(seed=42):
    random.seed(seed)  # Python éšæœºç§å­
    np.random.seed(seed)  # numpy éšæœºç§å­
    torch.manual_seed(seed)  # CPU torch éšæœºç§å­
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)  # GPU éšæœºç§å­
        torch.cuda.manual_seed_all(seed)  # å¤š GPU æƒ…å†µ

    # ä¿è¯ CUDA å¯å¤ç°ï¼ˆä½†å¯èƒ½ä¼šç•¥å¾®é™ä½é€Ÿåº¦ï¼‰
    if torch.backends.cudnn.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    os.environ['PYTHONHASHSEED'] = str(seed)


def save_model_checkpoint(model, optimizer, scheduler, config, save_dir, epoch, best_metric):
    os.makedirs(save_dir, exist_ok=True)

    torch.save(model.state_dict(), os.path.join(save_dir, "model.pt"))
    torch.save(optimizer.state_dict(), os.path.join(save_dir, "optimizer.pt"))
    torch.save(scheduler.state_dict(), os.path.join(save_dir, "scheduler.pt"))

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(vars(config), f, indent=2)

    with open(os.path.join(save_dir, "training_state.json"), "w") as f:
        json.dump({"epoch": epoch, "best_f1": best_metric}, f, indent=2)


def load_model_checkpoint(model, optimizer, scheduler, load_dir):
    device = next(model.parameters()).device
    model.load_state_dict(torch.load(os.path.join(load_dir, "model.pt"), map_location=device))
    optimizer.load_state_dict(torch.load(os.path.join(load_dir, "optimizer.pt"), map_location=device))
    scheduler.load_state_dict(torch.load(os.path.join(load_dir, "scheduler.pt"), map_location=device))

    with open(os.path.join(load_dir, "training_state.json")) as f:
        state = json.load(f)

    return state["epoch"], state["best_f1"]


def train(config):
    print("train config:", config)
    base_run_name = "{0}_train-{1}_{2}_{3}".format(
        datetime.now().strftime('%Y-%m-%d'),
        config.dataset_name,
        str(config.model),
        config.ex_name,
    )
    save_root = os.path.join(STORAGE_ROOT, "save_models")
    run_name = base_run_name
    save_dir = os.path.join(save_root, run_name)
    if os.path.exists(save_dir):
        idx = 1
        while True:
            run_name = "{0}_{1:03d}".format(base_run_name, idx)
            save_dir = os.path.join(save_root, run_name)
            if not os.path.exists(save_dir):
                break
            idx += 1
    tb_dir = os.path.join("/root/tf-logs", run_name)
    os.makedirs(tb_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=tb_dir)

    dev = getattr(config, "device", "cuda:0")
    if isinstance(dev, str) and (dev == "cuda" or dev.startswith("cuda")) and not torch.cuda.is_available():
        dev = "cpu"
    try:
        device = torch.device(dev)
    except Exception:
        device = torch.device("cpu")

    DATA_PATH = {
        "twitter2015": {
            # text data
            'train': os.path.join(DATA_ROOT, 'twitter2015/train.txt'),
            'valid': os.path.join(DATA_ROOT, 'twitter2015/valid.txt'),
            'test': os.path.join(DATA_ROOT, 'twitter2015/test.txt'),
        },
        "twitter2017": {
            # text data
            'train': os.path.join(DATA_ROOT, 'twitter2017/train.txt'),
            'valid': os.path.join(DATA_ROOT, 'twitter2017/valid.txt'),
            'test': os.path.join(DATA_ROOT, 'twitter2017/test.txt'),
        },
        "NewsMKG": {
            # text data generated by processor for NewsMKG
            'train': os.path.join(DATA_ROOT, 'NewsMKG/train.txt'),
            'valid': os.path.join(DATA_ROOT, 'NewsMKG/valid.txt'),
            'test': os.path.join(DATA_ROOT, 'NewsMKG/test.txt'),
        }
    }
    # image data
    IMG_PATH = {
        'twitter2015': os.path.join(DATA_ROOT, 'twitter2015/twitter2015_images'),
        'twitter2017': os.path.join(DATA_ROOT, 'twitter2017/twitter2017_images'),
        # å¯¹ NewsMKGï¼ŒIMGID å·²å« samples/xxx/yyyï¼Œè¿½åŠ  .jpg æ—¶ä½¿ç”¨æ­¤å‰ç¼€
        'NewsMKG': os.path.join(DATA_ROOT, 'NewsMKG'),
    }
    img_path = IMG_PATH[config.dataset_name]
    data_path = DATA_PATH[config.dataset_name]
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])])
    processor = MMPNERProcessor(data_path, config.text_encoder)
    train_dataset = MMPNERDataset(
        processor, transform, img_path=img_path, max_seq=config.max_len,
        sample_ratio=1.0, mode='train', set_prediction=True
    )
    train_loader = DataLoader(
        train_dataset, batch_size=config.batch_size, shuffle=True,
        num_workers=0,       # é¿å…å¤šè¿›ç¨‹å ç”¨ /dev/shm
        pin_memory=False,    # å…³é—­ pinned å†…å­˜ä»¥å‡è½»å†…å­˜å‹åŠ›
        collate_fn=collate_fn
    )

    val_dataset = MMPNERDataset(
        processor,
        transform,
        img_path=img_path,
        max_seq=config.max_len,
        sample_ratio=1.0,
        mode='valid',
        set_prediction=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,  # éªŒè¯é›†ä¸æ‰“ä¹±
        num_workers=0,       # é¿å…å¤šè¿›ç¨‹å ç”¨ /dev/shm
        pin_memory=False,
        collate_fn=collate_fn
    )

    config.num_labels = len(train_dataset.label_mapping)

    tokenizer = processor.tokenizer
    type_names = train_dataset.type_names

    start_epoch = 0
    best_f1 = 0.0
    model = MQSPNSetNER(config, tokenizer=tokenizer, type_names=type_names).to(device)
    
    if config.continue_train_name != "None":
        SAVE_ROOT = os.path.join(STORAGE_ROOT, "save_models")
        ckpt_dir = os.path.join(SAVE_ROOT, config.continue_train_name)

    no_decay = ["bias", "LayerNorm.weight", "LayerNorm.bias"]

    # ---- äº’æ–¥åˆ’åˆ†å‚æ•°ï¼šç¡®ä¿åŒä¸€ä¸ª tensor åªè½åœ¨ä¸€ä¸ªç»„é‡Œ ----
    param_roberta, param_downstream = [], []
    for n, p in model.named_parameters():
        if n.startswith("text_encoder."):
            param_roberta.append((n, p))
        else:
            param_downstream.append((n, p))

    # å°å·¥å…·ï¼šç»™æŸä¸€ç±»(named params)æŒ‰æœ‰/æ— weight_decayåˆ‡åˆ†ï¼Œè·³è¿‡ä¸éœ€è¦è®­ç»ƒçš„å‚æ•°
    def build_groups(named_params, lr, wd):
        params_decay = [p for n, p in named_params
                        if p.requires_grad and not any(nd in n for nd in no_decay)]
        params_nodec = [p for n, p in named_params
                        if p.requires_grad and any(nd in n for nd in no_decay)]
        groups = []
        if len(params_decay) > 0:
            groups.append({"params": params_decay, "weight_decay": wd, "lr": lr})
        if len(params_nodec) > 0:
            groups.append({"params": params_nodec, "weight_decay": 0.0, "lr": lr})
        return groups

    optimizer_grouped_parameters = []
    # RoBERTaï¼ˆæ–‡æœ¬ç¼–ç å™¨ï¼‰
    optimizer_grouped_parameters += build_groups(param_roberta,
                                                 config.fin_tuning_lr,
                                                 config.weight_decay_rate)
    # ä¸‹æ¸¸æ¨¡å—
    optimizer_grouped_parameters += build_groups(param_downstream,
                                                 config.downs_en_lr,
                                                 config.weight_decay_rate)

    # ---- å»é‡æ ¡éªŒï¼ˆé˜²æ­¢ä»»ä½•å‚æ•°å‡ºç°åœ¨å¤šä¸ªç»„ï¼‰----
    seen = set()
    for gi, g in enumerate(optimizer_grouped_parameters):
        for p in g["params"]:
            pid = id(p)
            if pid in seen:
                raise ValueError("parameter appears in multiple groups (group index {0})".format(gi))
            seen.add(pid)

    # ï¼ˆå¯é€‰ï¼‰æ‰“å°å¿«ç…§ï¼Œæ–¹ä¾¿ç¡®è®¤åˆ†ç»„æ˜¯å¦åˆç†
    def count_params(named_params):
        return sum(p.numel() for _, p in named_params)

    print("\n[DEBUG] param grouping snapshot")
    print(" - text_encoder tensors:", len(param_roberta), " params:", count_params(param_roberta))
    print(" - downstream tensors:", len(param_downstream), " params:", count_params(param_downstream))
    print()

    # optimizer = torch.optim.AdamW(optimizer_grouped_parameters)

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)
    t_total = len(train_loader) // config.gradient_accumulation_steps * config.epochs
    warmup_steps = int(t_total * config.warmup_prop)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)

    best_f1 = 0.0
    patience_counter = 0

    # =============== åŠ è½½ checkpointï¼ˆæ¨¡å‹æƒé‡ / æˆ–å®Œæ•´çŠ¶æ€ï¼‰ ===============
    if config.continue_train_name != "None":
        load_dir = ckpt_dir
        model.load_state_dict(torch.load(os.path.join(load_dir, "model.pt"), map_location=device))
        optimizer.load_state_dict(torch.load(os.path.join(load_dir, "optimizer.pt"), map_location=device))
        scheduler.load_state_dict(torch.load(os.path.join(load_dir, "scheduler.pt"), map_location=device))

        with open(os.path.join(load_dir, "training_state.json")) as f:
            state = json.load(f)
        start_epoch = state["epoch"]
        best_f1 = state["best_f1"]

    global_step = 0
    freeze_exist_epochs = 2
    try:
        for epoch in range(start_epoch, config.epochs + 1):
            if epoch < freeze_exist_epochs:
                for p in model.exist.parameters():
                    p.requires_grad = False
            else:
                for p in model.exist.parameters():
                    p.requires_grad = True
            
            model.train()
            total_loss = 0.0

            loop = tqdm(train_loader, desc="Epoch {0}/{1}".format(epoch, config.epochs), ncols=100)
            for step, batch in enumerate(loop):

            # input_ids = batch[0].to(device, non_blocking=True)
            # # token_type_ids = batch[1].to(device, non_blocking=True)
            # attention_mask = batch[1].to(device, non_blocking=True)
            # labels = batch[2].to(device, non_blocking=True)
            # image_tensor = batch[3].to(device, non_blocking=True)
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                images = batch.get("image", None)
                if images is not None:
                    images = images.to(device)

                targets = batch.get("targets", None)
                loss = model(
                    input_ids,
                    attention_mask,
                    image_tensor=images,
                    targets=targets,
                )
                if random.random() < 0.01:
                    print(f"loss_span={getattr(model, 'last_loss_span', None)} loss_region={getattr(model, 'last_loss_region', None)} loss_exist={getattr(model, 'last_loss_exist', None)}")

                loss = loss / config.gradient_accumulation_steps
                loss.backward()

            # æ¯ä¸ª step çš„ gradient norm å’Œ learning rate è®°å½•ï¼ˆåœ¨ optimizer.step ä¹‹å‰ï¼‰
                if (step + 1) % config.gradient_accumulation_steps == 0:
                    # è®¡ç®— grad_norm
                    total_norm = 0.0
                    for p in model.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** 0.5

                    #  clip æ¢¯åº¦ï¼Œä¼˜åŒ–å™¨æ­¥è¿›
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()

                    current_lr = scheduler.get_last_lr()[0]
                    writer.add_scalar("train/grad_norm", total_norm, global_step)
                    writer.add_scalar("train/learning_rate", current_lr, global_step)
                    global_step += 1

                total_loss += loss.item()
                loop.set_postfix(loss="{0:.4f}".format(loss.item()), lr=optimizer.param_groups[0]['lr'])

            avg_loss = total_loss / len(train_loader)
            writer.add_scalar("train/loss", avg_loss, epoch)
            print("\nâœ… Epoch {0} Train Loss: {1:.4f}".format(epoch, avg_loss))

        # f1, report = evaluate(model, val_loader, device, train_dataset.id2label)
            acc, f1, p, r = evaluate_model(
                model, val_loader, device, train_dataset.label_mapping,
                type_names=type_names,
                label_mapping=train_dataset.label_mapping
            )
            print(
                "ğŸ¯Epoch {0} Eval F1: {1:.4f} precision: {2:.4f} recall: {3:.4f} acc:{4:.4f}".format(epoch, f1, p, r, acc))
            writer.add_scalar("eval/f1", f1, epoch)
            writer.add_scalar("eval/precision", p, epoch)
            writer.add_scalar("eval/recall", r, epoch)
            writer.add_scalar("eval/acc", acc, epoch)

        # Early Stop
            if f1 > best_f1 + config.patience:
                best_f1 = f1
                patience_counter = 0
                save_model_checkpoint(model, optimizer, scheduler, config, save_dir, epoch, best_f1)
                print("âœ… Model saved to {0}".format(save_dir))
            else:
                patience_counter += 1
                print("ğŸ“‰ No improvement, patience {0}/{1}".format(patience_counter, config.patience_num))
                if epoch >= config.min_epoch_num and patience_counter >= config.patience_num:
                    print("â›”ï¸ Early stopping triggered.")
                    break
    finally:
        writer.close()


if __name__ == "__main__":
    from config import get_config

    set_seed(42)
    config = get_config()
    train(config)
