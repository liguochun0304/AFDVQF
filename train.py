# -*- coding: utf-8 -*-
# @Time    : 2025/7/22 ä¸‹åˆ1:13
# @Author  : liguochun
# @FileName: train.py
# @Software: PyCharm
# @Email   ï¼šliguochun0304@163.com
import json
import os
import random
from datetime import datetime

import numpy as np
import torch

from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup

from dataloader import MMPNERDataset, MMPNERProcessor, collate_fn
from model import build_model
from test import evaluate_model
from test import load_config

script_dir = os.path.dirname(os.path.abspath(__file__))


def set_seed(seed=42):
    random.seed(seed)  # Python éšæœºç§å­
    np.random.seed(seed)  # numpy éšæœºç§å­
    torch.manual_seed(seed)  # CPU torch éšæœºç§å­
    torch.cuda.manual_seed(seed)  # GPU éšæœºç§å­
    torch.cuda.manual_seed_all(seed)  # å¤š GPU æƒ…å†µ

    # ä¿è¯ CUDA å¯å¤ç°ï¼ˆä½†å¯èƒ½ä¼šç•¥å¾®é™ä½é€Ÿåº¦ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    os.environ['PYTHONHASHSEED'] = str(seed)


def save_model_checkpoint(model, optimizer, scheduler, config, save_dir, epoch, best_metric):
    os.makedirs(save_dir, exist_ok=True)

    torch.save(model.state_dict(), os.path.join(save_dir, "model.pt"))
    torch.save(optimizer.state_dict(), os.path.join(save_dir, "optimizer.pt"))
    torch.save(scheduler.state_dict(), os.path.join(save_dir, "scheduler.pt"))

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(vars(config), f, indent=2)

    with open(os.path.join(save_dir, "training_state.json"), "w") as f:
        json.dump({"epoch": epoch, "best_f1": best_metric}, f, indent=2)


def load_model_checkpoint(model, optimizer, scheduler, load_dir):
    model.load_state_dict(torch.load(os.path.join(load_dir, "model.pt")))
    optimizer.load_state_dict(torch.load(os.path.join(load_dir, "optimizer.pt")))
    scheduler.load_state_dict(torch.load(os.path.join(load_dir, "scheduler.pt")))

    with open(os.path.join(load_dir, "training_state.json")) as f:
        state = json.load(f)

    return state["epoch"], state["best_f1"]


def train(config):
    print("train config:", config)
    run_name = "{0}_train-{1}_{2}_{3}".format(datetime.now().strftime('%Y-%m-%d'), config.dataset_name, str(config.model), config.ex_name)
    save_dir = os.path.join(script_dir, "save_models", "{0}".format(run_name))
    tb_dir = os.path.join("/root/tf-logs", run_name)
    os.makedirs(tb_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=tb_dir)

    device = torch.device(config.device)

    DATA_PATH = {
        "twitter2015": {
            # text data
            'train': os.path.join(script_dir, 'data/twitter2015/train.txt'),
            'valid': os.path.join(script_dir, 'data/twitter2015/valid.txt'),
            'test': os.path.join(script_dir, 'data/twitter2015/test.txt'),
        },
        "twitter2017": {
            # text data
            'train': os.path.join(script_dir, 'data/twitter2017/train.txt'),
            'valid': os.path.join(script_dir, 'data/twitter2017/valid.txt'),
            'test': os.path.join(script_dir, 'data/twitter2017/test.txt'),
        },
        "NewsMKG": {
            # text data generated by processor for NewsMKG
            'train': os.path.join(script_dir, 'data/NewsMKG/train.txt'),
            'valid': os.path.join(script_dir, 'data/NewsMKG/valid.txt'),
            'test': os.path.join(script_dir, 'data/NewsMKG/test.txt'),
        }
    }
    # image data
    IMG_PATH = {
        'twitter2015': os.path.join(script_dir, 'data/twitter2015/twitter2015_images'),
        'twitter2017': os.path.join(script_dir, 'data/twitter2017/twitter2017_images'),
        # å¯¹ NewsMKGï¼ŒIMGID å·²å« samples/xxx/yyyï¼Œè¿½åŠ  .jpg æ—¶ä½¿ç”¨æ­¤å‰ç¼€
        'NewsMKG': os.path.join(script_dir, 'data/NewsMKG'),
    }
    img_path = IMG_PATH[config.dataset_name]
    data_path = DATA_PATH[config.dataset_name]
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])])
    processor = MMPNERProcessor(data_path, config.text_encoder)
    train_dataset = MMPNERDataset(
        processor, transform, img_path=img_path, max_seq=config.max_len,
        sample_ratio=1.0, mode='train'
    )
    train_loader = DataLoader(
        train_dataset, batch_size=64, shuffle=True,
        num_workers=0,       # é¿å…å¤šè¿›ç¨‹å ç”¨ /dev/shm
        pin_memory=False,    # å…³é—­ pinned å†…å­˜ä»¥å‡è½»å†…å­˜å‹åŠ›
        collate_fn=collate_fn
    )

    val_dataset = MMPNERDataset(
        processor,
        transform,
        img_path=img_path,
        max_seq=config.max_len,
        sample_ratio=1.0,
        mode='valid'
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,  # éªŒè¯é›†ä¸æ‰“ä¹±
        num_workers=0,       # é¿å…å¤šè¿›ç¨‹å ç”¨ /dev/shm
        pin_memory=False,
        collate_fn=collate_fn
    )

    config.num_labels = len(train_dataset.label_mapping)

    start_epoch = 0
    best_f1 = 0.0
    if config.continue_train_name == "None":
        # æ­£å¸¸é¦–æ¬¡è®­ç»ƒ
        model = build_model(config).to(device)
    else:
        SAVE_ROOT = os.path.join(script_dir, "save_models")
        # ä» save_models/<name> è¯»å–å…ˆå‰ config ä»¥ç¡®ä¿ç»“æ„ä¸€è‡´
        ckpt_dir = os.path.join(SAVE_ROOT, config.continue_train_name)
        prev_cfg = load_config(config.continue_train_name)  # ä½ å·²æœ‰çš„å‡½æ•°

        # ç”¨å…ˆå‰é…ç½®æ„å»ºç»“æ„ï¼Œä½†è¿è¡Œå‚æ•°æ²¿ç”¨å½“å‰å‘½ä»¤è¡Œï¼ˆæ¯”å¦‚æ–°çš„ LR/drop/align ç­‰ï¼‰
        model = build_model(prev_cfg).to(device)

    no_decay = ["bias", "LayerNorm.weight", "LayerNorm.bias"]

    # ---- äº’æ–¥åˆ’åˆ†å‚æ•°ï¼šç¡®ä¿åŒä¸€ä¸ª tensor åªè½åœ¨ä¸€ä¸ªç»„é‡Œ ----
    param_roberta, param_clip, param_downstream = [], [], []
    for n, p in model.named_parameters():
        if n.startswith("text_encoder."):
            param_roberta.append((n, p))
        elif n.startswith("clip_vision.") or n.startswith("clip.vision_model."):
            # å…¼å®¹ä¸¤ç§å‘½åï¼šä½ è‡ªå·±æ¨¡å—é‡Œçš„ clip_vision.*ï¼Œä»¥åŠ transformers çš„ clip.vision_model.*
            param_clip.append((n, p))
        else:
            param_downstream.append((n, p))

    # å°å·¥å…·ï¼šç»™æŸä¸€ç±»(named params)æŒ‰æœ‰/æ— weight_decayåˆ‡åˆ†ï¼Œè·³è¿‡ä¸éœ€è¦è®­ç»ƒçš„å‚æ•°
    def build_groups(named_params, lr, wd):
        params_decay = [p for n, p in named_params
                        if p.requires_grad and not any(nd in n for nd in no_decay)]
        params_nodec = [p for n, p in named_params
                        if p.requires_grad and any(nd in n for nd in no_decay)]
        groups = []
        if len(params_decay) > 0:
            groups.append({"params": params_decay, "weight_decay": wd, "lr": lr})
        if len(params_nodec) > 0:
            groups.append({"params": params_nodec, "weight_decay": 0.0, "lr": lr})
        return groups

    optimizer_grouped_parameters = []
    # RoBERTaï¼ˆæ–‡æœ¬ç¼–ç å™¨ï¼‰
    optimizer_grouped_parameters += build_groups(param_roberta,
                                                 config.fin_tuning_lr,
                                                 config.weight_decay_rate)
    # ä¸‹æ¸¸æ¨¡å—
    optimizer_grouped_parameters += build_groups(param_downstream,
                                                 config.downs_en_lr,
                                                 config.weight_decay_rate)
    # CLIPè§†è§‰å¡”ï¼ˆä»…åœ¨ --vision_trainable æ—¶åŠ å…¥ä¼˜åŒ–å™¨ï¼‰
    if getattr(config, "vision_trainable", False):
        optimizer_grouped_parameters += build_groups(param_clip,
                                                     config.clip_lr,
                                                     config.weight_decay_rate)

    # ---- å»é‡æ ¡éªŒï¼ˆé˜²æ­¢ä»»ä½•å‚æ•°å‡ºç°åœ¨å¤šä¸ªç»„ï¼‰----
    seen = set()
    for gi, g in enumerate(optimizer_grouped_parameters):
        for p in g["params"]:
            pid = id(p)
            if pid in seen:
                raise ValueError("parameter appears in multiple groups (group index {0})".format(gi))
            seen.add(pid)

    # ï¼ˆå¯é€‰ï¼‰æ‰“å°å¿«ç…§ï¼Œæ–¹ä¾¿ç¡®è®¤åˆ†ç»„æ˜¯å¦åˆç†
    def count_params(named_params):
        return sum(p.numel() for _, p in named_params)

    print("\n[DEBUG] param grouping snapshot")
    print(" - roberta  tensors:", len(param_roberta), " params:", count_params(param_roberta))
    print(" - clip     tensors:", len(param_clip), " params:", count_params(param_clip),
          " (vision_trainable=", getattr(config, "vision_trainable", False), ")")
    print(" - downstream tensors:", len(param_downstream), " params:", count_params(param_downstream))
    print()

    # optimizer = torch.optim.AdamW(optimizer_grouped_parameters)

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)
    t_total = len(train_loader) // config.gradient_accumulation_steps * config.epochs
    warmup_steps = int(t_total * config.warmup_prop)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)

    best_f1 = 0.0
    patience_counter = 0

    # =============== åŠ è½½ checkpointï¼ˆæ¨¡å‹æƒé‡ / æˆ–å®Œæ•´çŠ¶æ€ï¼‰ ===============
    if config.continue_train_name != "None":
        load_dir = ckpt_dir
        model.load_state_dict(torch.load(os.path.join(load_dir, "model.pt")))
        optimizer.load_state_dict(torch.load(os.path.join(load_dir, "optimizer.pt")))
        scheduler.load_state_dict(torch.load(os.path.join(load_dir, "scheduler.pt")))

        with open(os.path.join(load_dir, "training_state.json")) as f:
            state = json.load(f)
        start_epoch = state["epoch"]
        best_f1 = state["best_f1"]

    global_step = 0
    try:
        for epoch in range(start_epoch, config.epochs + 1):
            model.train()
            total_loss = 0.0

            loop = tqdm(train_loader, desc="Epoch {0}/{1}".format(epoch, config.epochs), ncols=100)
            for step, batch in enumerate(loop):

            # input_ids = batch[0].to(device, non_blocking=True)
            # # token_type_ids = batch[1].to(device, non_blocking=True)
            # attention_mask = batch[1].to(device, non_blocking=True)
            # labels = batch[2].to(device, non_blocking=True)
            # image_tensor = batch[3].to(device, non_blocking=True)
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)
                images = batch.get("image", None)
                if images is not None:
                    images = images.to(device)

                loss = model(
                    input_ids,
                    attention_mask,
                    image_tensor=images,
                    labels=labels,
                )

                loss = loss / config.gradient_accumulation_steps
                loss.backward()

            # æ¯ä¸ª step çš„ gradient norm å’Œ learning rate è®°å½•ï¼ˆåœ¨ optimizer.step ä¹‹å‰ï¼‰
                if (step + 1) % config.gradient_accumulation_steps == 0:
                    # è®¡ç®— grad_norm
                    total_norm = 0.0
                    for p in model.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** 0.5

                    #  clip æ¢¯åº¦ï¼Œä¼˜åŒ–å™¨æ­¥è¿›
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()

                    current_lr = scheduler.get_last_lr()[0]
                    writer.add_scalar("train/grad_norm", total_norm, global_step)
                    writer.add_scalar("train/learning_rate", current_lr, global_step)
                    global_step += 1

                total_loss += loss.item()
                loop.set_postfix(loss="{0:.4f}".format(loss.item()), lr=optimizer.param_groups[0]['lr'])

            avg_loss = total_loss / len(train_loader)
            writer.add_scalar("train/loss", avg_loss, epoch)
            print("\nâœ… Epoch {0} Train Loss: {1:.4f}".format(epoch, avg_loss))

        # f1, report = evaluate(model, val_loader, device, train_dataset.id2label)
            acc, f1, p, r = evaluate_model(model, val_loader, device, train_dataset.label_mapping)
            print(
                "ğŸ¯Epoch {0} Eval F1: {1:.4f} precision: {2:.4f} recall: {3:.4f} acc:{4:.4f}".format(epoch, f1, p, r, acc))
            writer.add_scalar("eval/f1", f1, epoch)
            writer.add_scalar("eval/precision", p, epoch)
            writer.add_scalar("eval/recall", r, epoch)
            writer.add_scalar("eval/acc", acc, epoch)

        # Early Stop
            if f1 > best_f1 + config.patience:
                best_f1 = f1
                patience_counter = 0
                save_model_checkpoint(model, optimizer, scheduler, config, save_dir, epoch, best_f1)
                print("âœ… Model saved to {0}".format(save_dir))
            else:
                patience_counter += 1
                print("ğŸ“‰ No improvement, patience {0}/{1}".format(patience_counter, config.patience_num))
                if epoch >= config.min_epoch_num and patience_counter >= config.patience_num:
                    print("â›”ï¸ Early stopping triggered.")
                    break
    finally:
        writer.close()


if __name__ == "__main__":
    from config import get_config

    set_seed(42)
    config = get_config()
    train(config)
