# -*- coding: utf-8 -*-
# @Time    : 2025/7/21 ä¸‹åˆ9:36
# @Author  : liguochun
# @FileName: model.py
# @Software: PyCharm
# @E-mail  : liguochun0304@163.com


import torch.nn as nn
from torchcrf import CRF
from transformers import RobertaModel
from transformers import RobertaTokenizer, RobertaModel
class MultimodalNER(nn.Module):
    def __init__(self,
                 text_encoder_path="roberta-base",
                 image_encoder_path="clip-patch32",
                 num_labels=9,
                 hidden_dim=256):
        super(MultimodalNER, self).__init__()

        # æ–‡æœ¬ç¼–ç å™¨ï¼ˆRoBERTaï¼‰
        self.roberta = RobertaModel.from_pretrained(text_encoder_path)
        self.text_hidden_size = self.roberta.config.hidden_size  # ä¸€èˆ¬ä¸º768

        # å›¾åƒç¼–ç å™¨ï¼ˆCLIPï¼‰
        self.clip = CLIPModel.from_pretrained(image_encoder_path)
        self.clip.eval()  # æ¨ç†æ¨¡å¼ï¼Œé˜²æ­¢dropoutç­‰

        # è·å– projection_dimï¼ˆå¦‚512ï¼‰ï¼Œæ³¨æ„ä¸æ˜¯ vision_model.config.hidden_sizeï¼ˆå¦‚768ï¼‰
        self.image_hidden_size = self.clip.config.projection_dim  # âœ… æ­£ç¡®ï¼šä¸º get_image_features çš„è¾“å‡ºç»´åº¦
        self.clip_proj = nn.Linear(self.image_hidden_size, self.text_hidden_size)

        # BiLSTM
        self.bilstm = nn.LSTM(input_size=self.text_hidden_size * 2,
                              hidden_size=hidden_dim,
                              num_layers=1,
                              bidirectional=True,
                              batch_first=True)

        # åˆ†ç±»å™¨ + CRF
        self.classifier = nn.Linear(hidden_dim * 2, num_labels)
        self.crf = CRF(num_labels, batch_first=True)

    def forward(self, input_ids, attention_mask, image_tensor, labels=None):
        """
        input_ids: [B, T]
        attention_mask: [B, T]
        image_tensor: [B, 3, 224, 224] - ä½¿ç”¨ CLIPProcessor é¢„å¤„ç†
        labels: [B, T] (optional)
        """
        # 1. æ–‡æœ¬ç‰¹å¾æå–
        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        text_feat = roberta_output.last_hidden_state  # [B, T, H]

        # 2. å›¾åƒç‰¹å¾æå–
        with torch.no_grad():
            image_feat = self.clip.get_image_features(pixel_values=image_tensor)  # [B, 512]
        image_feat = self.clip_proj(image_feat)  # [B, 768]
        image_feat = image_feat.unsqueeze(1).repeat(1, text_feat.size(1), 1)  # [B, T, 768]

        # 3. æ‹¼æ¥å›¾æ–‡ç‰¹å¾
        fused_feat = torch.cat([text_feat, image_feat], dim=-1)  # [B, T, 2*768]

        # 4. BiLSTM -> Linear -> CRF
        lstm_out, _ = self.bilstm(fused_feat)  # [B, T, 2*hidden_dim]
        emissions = self.classifier(lstm_out)  # [B, T, num_labels]

        # 5. CRFè®­ç»ƒæˆ–è§£ç 
        if labels is not None:
            mask = attention_mask.bool()
            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')
            return loss
        else:
            pred = self.crf.decode(emissions, mask=attention_mask.bool())  # List[List[int]]
            return pred


if __name__ == "__main__":
    import torch
    from transformers import CLIPProcessor, CLIPModel,CLIPModel, CLIPProcessor, BertTokenizer,CLIPModel
    from PIL import Image
    import os
    from transformers import RobertaTokenizer

    # ğŸ”§ å‚æ•°é…ç½®
    device = torch.device("cuda:5" if torch.cuda.is_available() else "cpu")
    roberta_name = "roberta-base"
    clip_name = "clip-patch32"

    # ğŸ§  åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
    tokenizer = RobertaTokenizer.from_pretrained(roberta_name)

    clip_model = CLIPModel.from_pretrained(clip_name).to(device)
    clip_processor = CLIPProcessor.from_pretrained(clip_name)



    # ğŸ‘€ æµ‹è¯•æ•°æ®
    test_text = "Let ' s go for all @ warriors ğŸ’ª ğŸ€ ğŸ€ for Erik # minicurry # bball ğŸ€ ğŸ€ # NBAFinals http://t.co/ustuUYZ2T3"
    test_image_path = "data/MORE/img_org/train/0b982f1d-df6d-5053-8486-147eaaefe0a7.jpg"
    assert os.path.exists(test_image_path), "è¯·ç¡®ä¿ test.jpg å›¾åƒæ–‡ä»¶å­˜åœ¨ï¼"

    # âœï¸ æ–‡æœ¬ç¼–ç 
    encoded = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True)
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    # ğŸ–¼ï¸ å›¾åƒé¢„å¤„ç†
    image = Image.open(test_image_path).convert("RGB")
    image_inputs = clip_processor(images=image, return_tensors="pt").to(device)  # [1, 3, 224, 224]

    # æå–å›¾åƒç‰¹å¾ï¼ˆåªæè§†è§‰éƒ¨åˆ†ï¼‰
    with torch.no_grad():
        image_feat = clip_model.get_image_features(pixel_values=image_inputs["pixel_values"])  # [1, D]

    # ğŸ“¦ åŠ è½½ä½ çš„æ¨¡å‹ï¼ˆç¡®ä¿æ¨¡å‹ç±»ä½¿ç”¨ transformers.CLIPModel ç‰¹å¾ç»´åº¦ï¼‰
    model = MultimodalNER().to(device)
    model.eval()

    # ğŸ¤– æ¨¡å‹æ¨ç†ï¼ˆä¸å¸¦æ ‡ç­¾ï¼Œè¾“å‡ºé¢„æµ‹ï¼‰
    with torch.no_grad():
        pred_tags = model(input_ids=input_ids, attention_mask=attention_mask, image_tensor=image_inputs["pixel_values"])

    # ğŸ§¾ æ ‡ç­¾æ˜ å°„ï¼ˆç¤ºä¾‹ï¼‰
    id2label = {
        0: 'O', 1: 'B-LOC', 2: 'I-LOC',
        3: 'B-ORG', 4: 'I-ORG',
        5: 'B-PER', 6: 'I-PER',
        7: 'B-MISC', 8: 'I-MISC'
    }

    print("ğŸ§© é¢„æµ‹æ ‡ç­¾ IDï¼š", pred_tags[0])
    print("ğŸ§¾ é¢„æµ‹æ ‡ç­¾ï¼š", [id2label.get(i, 'UNK') for i in pred_tags[0]])

    # ğŸ§± å¯é€‰ï¼šå±•ç¤º token å’Œå¯¹åº”é¢„æµ‹
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    print("\nğŸ“‹ Token å¯¹åº”é¢„æµ‹ï¼š")
    for token, label_id in zip(tokens, pred_tags[0]):
        print(f"{token:15} â†’ {id2label.get(label_id, 'UNK')}")
