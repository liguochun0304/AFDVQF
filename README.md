# MNER

本仓库用于开发多模态命名体识别算法的训练工作
创新点1. roberta-bilstm-crf 融合 clip 的 多模态识别算法
创新点2. 实体联合对齐？？

## 简单实验

```text

         [文本] → RoBERTa → BiLSTM →     ┐
                                        ├→ 拼接 → 线性层 → CRF → 预测标签
[图像] → CLIP(图像编码) → 映射/池化 →       ┘

```


非常好的问题。你的三个优化方向：

1. ✅ 加一层 **FeedForward（TransformerEncoder或MLP）**
2. ✅ 在 BiLSTM 后加 **Self-Attention 层**
3. ✅ 将 GMF 换成 **非线性 MLP 融合**

它们不是互斥的，但建议你**逐步引入**、**逐个评估效果**。下面我详细分析每一项的目标、相互影响以及最佳引入顺序：

---

## 🔍 三个改进点的定位与兼容性


| 改进项                                     | 作用位置         | 提升目标             | 与其他模块兼容性                    | 推荐顺序                 |
| ------------------------------------------ | ---------------- | -------------------- | ----------------------------------- | ------------------------ |
| ✅ GMF → MLP融合                          | 图文融合层       | 增强模态对齐能力     | ✅ 兼容全部                         | 🥇**建议第一步尝试**     |
| ✅ 加 FeedForward 层（MLP or Transformer） | BiLSTM后、分类前 | 增强文本建模深度     | ✅ 可与其他共存                     | 🥈 第二步                |
| ✅ BiLSTM 后加 Self-Attention              | BiLSTM输出后     | 强化长程依赖信息建模 | ⚠️ 与 MLP/Transformer功能重叠较大 | 🥉 最后尝试（或替代FFN） |

---

## ✅ 推荐策略：逐步叠加 + 监控效果

你可以按如下顺序逐步引入，每一步观察 dev F1 / loss 是否提升：

1. **替换 GMF 为非线性 MLP 融合（低风险，常有效）**
2. **在 BiLSTM 后增加一个 MLP（两层Linear+ReLU）或 TransformerEncoder（适合长文本）**
3. **在 BiLSTM 输出上加 self-attention 层（可替代第2步，但需注意过拟合）**

最终结构可以是：

```
CLIP + RoBERTa
  ↓
MLP融合图文特征
  ↓
BiLSTM
  ↓
TransformerEncoder or MLP
  ↓
MLP Head + CRF
```

---

## ❗ 为什么不建议“一起上”？

* 🧪 一起上，难以定位哪一层有效；
* 🚨 多层并上风险大，训练变慢、可能过拟合；
* 📉 若性能下降，也不易定位是哪个模块导致。

---

## ✅ 可视化建议

你用的是 SwanLab，可以这样监控每次尝试：

* 每个尝试建立一个 run（记录结构、日期）
* log：
  * `train/loss`、`eval/f1`、`train/grad_norm`
  * 结构描述放在备注中

---

如果你愿意，我可以先帮你把 “GMF → MLP融合” 模块写好，并和你现有模型对接起来，你只要一行替换就行。是否需要我来写这一部分？
