# -*- coding: utf-8 -*-
# @Time    : 2025/7/24 ä¸Šåˆ9:10
# @Author  : liguochun
# @FileName: test.py
# @Software: PyCharm
# @Email   ï¼šliguochun0304@163.com
# test.py
import argparse
import json
import os

import torch
from seqeval.metrics import classification_report as seq_classification_report
from seqeval.metrics import f1_score as seq_f1_score
from torch.utils.data import DataLoader
from transformers import RobertaTokenizer, CLIPProcessor
from metrics import evaluate_each_class, evaluate
from dataloader import MultimodalNERDataset, collate_fn
from model import MultimodalNER
from transformers import BertConfig
from transformers import BertTokenizer
script_dir = os.path.dirname(os.path.abspath(__file__))


# def evaluate(model, val_loader, device, id2label):
#     model.eval()
#     all_preds, all_labels = [], []
#
#     with torch.no_grad():
#         for batch in val_loader:
#             input_ids = batch["input_ids"].to(device)
#             attention_mask = batch["attention_mask"].to(device)
#             labels = batch["labels"].to(device)
#             image_tensor = batch["image_tensor"].to(device)
#
#             # é¢„æµ‹çš„æ ‡ç­¾ id åºåˆ—
#             preds = model(input_ids, attention_mask, image_tensor)
#
#             for p_ids, l_ids, mask in zip(preds, labels, attention_mask):
#                 valid_len = mask.sum().item()
#                 # æˆªå–æœ‰æ•ˆ tokenï¼Œæ˜ å°„æˆæ ‡ç­¾å­—ç¬¦ä¸²
#                 pred_labels = [id2label[i] for i in p_ids[:valid_len]]
#                 true_labels = [id2label[i.item()] for i in l_ids[:valid_len]]
#
#                 all_preds.append(pred_labels)
#                 all_labels.append(true_labels)
#
#     # å®ä½“çº§åˆ«è¯„ä¼°
#     f1 = seq_f1_score(all_labels, all_preds)
#     report = seq_classification_report(all_labels, all_preds, zero_division=0, digits=4, output_dict=True)
#     return f1, report


def evaluate_model(model, val_loader, device, tags):
    """
    è°ƒç”¨å·²æœ‰ get_chunks é€»è¾‘çš„å®Œæ•´è¯„ä¼°æµç¨‹ï¼ˆåŒ…æ‹¬æ•´ä½“å’Œæ¯ä¸ªç±»åˆ«çš„ F1/P/Rï¼‰
    """
    model.eval()
    all_preds, all_labels, all_words = [], [], []

    idx2tag = {v: k for k, v in tags.items()}

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            image_tensor = batch["image_tensor"].to(device)

            preds = model(input_ids, attention_mask, image_tensor)  # List[List[int]]

            for p_ids, l_ids, mask, token_ids in zip(preds, labels, attention_mask, input_ids):
                valid_len = mask.sum().item()
                all_preds.append(p_ids[:valid_len])
                all_labels.append([lid.item() for lid in l_ids[:valid_len]])
                all_words.append([wid.item() for wid in token_ids[:valid_len]])

    # ä½¿ç”¨å·²æœ‰é€»è¾‘è¯„ä¼°å®ä½“çº§æŒ‡æ ‡
    acc, f1, p, r = evaluate(all_preds, all_labels, all_words, tags)

    print(f"[Overall] Acc={acc:.4f}, P={p:.4f}, R={r:.4f}, F1={f1:.4f}")

    # æ¯ä¸ªç±»åˆ«å•ç‹¬è¯„ä¼°
    entity_types = sorted(set(tag.split('-')[-1] for tag in tags if '-' in tag))
    for ent_type in entity_types:
        f1_c, p_c, r_c = evaluate_each_class(all_preds, all_labels, all_words, tags, ent_type)
        print(f"[{ent_type}] P={p_c:.4f}, R={r_c:.4f}, F1={f1_c:.4f}")
    return acc, f1, p, r

def load_config(model_dir):
    config_path = os.path.join(script_dir, "save_models", model_dir, "config.json")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_path}")
    with open(config_path, "r") as f:
        config_dict = json.load(f)
    return argparse.Namespace(**config_dict)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--save_name", type=str, required=True, help="ä¿å­˜æ¨¡å‹name")
    parser.add_argument("--device", type=str, default="cuda:0")
    args = parser.parse_args()

    config = load_config(args.save_name)
    config.device = args.device
    device = torch.device(config.device)

    if config.text_encoder == "bert-base-uncased":
        tokenizer = BertTokenizer.from_pretrained(os.path.join(script_dir, config.text_encoder))
    else:
        tokenizer = RobertaTokenizer.from_pretrained(os.path.join(script_dir, config.text_encoder))
    processor = CLIPProcessor.from_pretrained(os.path.join(script_dir, config.image_encoder))

    test_dataset = MultimodalNERDataset(config.dataset_name, tokenizer, processor, config.max_len, dataset_type="test")
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)

    # model = MultimodalNER(text_encoder_path=config.text_encoder, use_image=config.use_image).to(device)
    
    model = MultimodalNER(num_labels=len(test_dataset.id2label), text_encoder_path=config.text_encoder,
                          use_image=config.use_image,
                          fusion_type=config.fusion_type,
                          use_coattention=config.use_coattention).to(device)
    model_path = os.path.join(script_dir, "save_models", args.save_name, "model.pt")
    model.load_state_dict(torch.load(model_path, map_location=device))

    acc, f1, p, r = evaluate_model(model, test_loader, device, test_dataset.label2id)
    # print(f"\nğŸ“Š Test F1-score: {f1:.4f}")
    # print("ğŸ“‹ Classification Report:")
    # print(json.dumps(report, indent=2, ensure_ascii=False,default=str))


if __name__ == "__main__":
    main()
